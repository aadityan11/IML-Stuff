{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device( 'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_TYPE = 'vit_h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(\"HOME:\", HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
    "print(CHECKPOINT_PATH, \"; exist:\", os.path.isfile(CHECKPOINT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_generator = SamAutomaticMaskGenerator(\n",
    "    model=sam,\n",
    "    points_per_side=32,\n",
    "    pred_iou_thresh=0.93,\n",
    "    stability_score_thresh=0.98,\n",
    "    crop_n_layers=1,\n",
    "    crop_n_points_downscale_factor=2,\n",
    "    min_mask_region_area=100,  # Requires open-cv to run post-processing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "IMAGE_NAME = \"images.jpeg\"\n",
    "IMAGE_PATH = os.path.join(HOME, \"segmentation\", IMAGE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import supervision as sv\n",
    "\n",
    "image_bgr = cv2.imread(IMAGE_PATH)\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "sam_result = mask_generator.generate(image_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sam_result[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "detections = sv.Detections.from_sam(sam_result=sam_result)\n",
    "\n",
    "annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=[image_bgr, annotated_image],\n",
    "    grid_size=(1, 2),\n",
    "    titles=['source image', 'segmented image']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sam_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = [\n",
    "    mask['segmentation']\n",
    "    for mask\n",
    "    in sorted(sam_result, key=lambda x: x['area'], reverse=True)\n",
    "]\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=masks,\n",
    "    grid_size=(rows, cols),\n",
    "    size=(16, 16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Number of images\n",
    "num_images = len(masks)\n",
    "\n",
    "# Calculate the number of rows and columns\n",
    "rows = math.ceil(num_images / 10)  # Ensure enough rows to fit all images\n",
    "cols = min(10, num_images)  # Maximum 10 columns, adjust if fewer images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# 'sam_result' is a list of dicts, one per detected mask.\n",
    "# Each dict has keys like 'segmentation', 'bbox', 'area', etc.\n",
    "# For example: sam_result[i]['bbox'] might look like [x, y, w, h]\n",
    "\n",
    "# Convert the image back to BGR if needed (it already is in BGR as `image_bgr` from your code).\n",
    "output_image = image_bgr.copy()\n",
    "\n",
    "# Iterate over all masks and draw their bounding boxes\n",
    "for mask_data in sam_result:\n",
    "    bbox = mask_data['bbox']  # [x, y, w, h]\n",
    "    \n",
    "    # Convert coordinates to int\n",
    "    x, y, w, h = map(int, bbox)\n",
    "    \n",
    "    cv2.rectangle(\n",
    "        img=output_image,\n",
    "        pt1=(x, y),\n",
    "        pt2=(x + w, y + h),\n",
    "        color=(0, 255, 0),\n",
    "        thickness=2\n",
    "    )\n",
    "\n",
    "# Optionally, you can display the image using OpenCV\n",
    "# Note: if you're running in a notebook or a headless environment, \n",
    "# you might need to use matplotlib or other methods to display the image.\n",
    "cv2.imshow(\"Objects with Bounding Boxes\", output_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "IMAGE_NAME = \"dog.jpeg\"\n",
    "IMAGE_PATH = os.path.join(HOME, \"data\", IMAGE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m box_annotator \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mBoxAnnotator(color\u001b[38;5;241m=\u001b[39msv\u001b[38;5;241m.\u001b[39mColor\u001b[38;5;241m.\u001b[39mRED, color_lookup\u001b[38;5;241m=\u001b[39msv\u001b[38;5;241m.\u001b[39mColorLookup\u001b[38;5;241m.\u001b[39mINDEX)\n\u001b[1;32m      2\u001b[0m mask_annotator \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mMaskAnnotator(color\u001b[38;5;241m=\u001b[39msv\u001b[38;5;241m.\u001b[39mColor\u001b[38;5;241m.\u001b[39mRED, color_lookup\u001b[38;5;241m=\u001b[39msv\u001b[38;5;241m.\u001b[39mColorLookup\u001b[38;5;241m.\u001b[39mINDEX)\n\u001b[1;32m      4\u001b[0m detections \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mDetections(\n\u001b[0;32m----> 5\u001b[0m     xyxy\u001b[38;5;241m=\u001b[39m\u001b[43msv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_to_xyxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      6\u001b[0m     mask\u001b[38;5;241m=\u001b[39mmasks\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m detections \u001b[38;5;241m=\u001b[39m detections[detections\u001b[38;5;241m.\u001b[39marea \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(detections\u001b[38;5;241m.\u001b[39marea)]\n\u001b[1;32m     10\u001b[0m source_image \u001b[38;5;241m=\u001b[39m box_annotator\u001b[38;5;241m.\u001b[39mannotate(scene\u001b[38;5;241m=\u001b[39mimage_bgr\u001b[38;5;241m.\u001b[39mcopy(), detections\u001b[38;5;241m=\u001b[39mdetections)\n",
      "File \u001b[0;32m~/Coding/IML Final Project/.venv/lib/python3.10/site-packages/supervision/detection/utils.py:318\u001b[0m, in \u001b[0;36mmask_to_xyxy\u001b[0;34m(masks)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmask_to_xyxy\u001b[39m(masks: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    307\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m    Converts a 3D `np.array` of 2D bool masks into a 2D `np.array` of bounding boxes.\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m            `(x_min, y_min, x_max, y_max)` for each mask\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[43mmasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    319\u001b[0m     xyxy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n, \u001b[38;5;241m4\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, mask \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(masks):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import supervision as sv\n",
    "import cv2\n",
    "\n",
    "# Extract masks from the SAM result and sort them by area (descending)\n",
    "masks = [\n",
    "    mask_data['segmentation']\n",
    "    for mask_data in sorted(sam_result, key=lambda x: x['area'], reverse=True)\n",
    "]\n",
    "\n",
    "# Convert the masks to bounding boxes in xyxy format\n",
    "xyxy_boxes = sv.mask_to_xyxy(masks=masks)\n",
    "\n",
    "# Create a Detections object from the bounding boxes and masks\n",
    "detections = sv.Detections(\n",
    "    xyxy=xyxy_boxes,\n",
    "    mask=masks\n",
    ")\n",
    "\n",
    "# (Optional) If you only want to display the largest detected object,\n",
    "# filter detections by their area to select the largest one.\n",
    "detections = detections[detections.area == np.max(detections.area)]\n",
    "\n",
    "# Create annotators for boxes and masks\n",
    "box_annotator = sv.BoxAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n",
    "mask_annotator = sv.MaskAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "# Annotate the original image with bounding boxes and masks\n",
    "source_image = box_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "segmented_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "\n",
    "# Visualize the annotated results side-by-side in a grid\n",
    "sv.plot_images_grid(\n",
    "    images=[source_image, segmented_image],\n",
    "    grid_size=(1, 2),\n",
    "    titles=['Source Image', 'Segmented Image']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_bgr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mimage_bgr\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage at path \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m could not be loaded. Check the file path.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_bgr' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
